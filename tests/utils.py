"""Test utilities and helper functions."""

import os
import time
import functools
import contextlib
from typing import Any, Callable, Dict, List, Optional, Union
from pathlib import Path

import torch
import pytest
import numpy as np


def requires_gpu(func: Callable) -> Callable:
    """Decorator to skip tests if GPU is not available."""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        if not torch.cuda.is_available():
            pytest.skip("GPU not available")
        return func(*args, **kwargs)
    return wrapper


def requires_large_memory(min_gb: float = 8.0) -> Callable:
    """Decorator to skip tests if insufficient memory available."""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            try:
                import psutil
                memory_gb = psutil.virtual_memory().total / (1024**3)
                if memory_gb < min_gb:
                    pytest.skip(f"Insufficient memory: {memory_gb:.1f}GB < {min_gb}GB")
            except ImportError:
                pytest.skip("psutil not available for memory check")
            return func(*args, **kwargs)
        return wrapper
    return decorator


def timeout(seconds: float) -> Callable:
    """Decorator to add timeout to test functions."""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError(f"Test {func.__name__} timed out after {seconds}s")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(int(seconds))
            
            try:
                result = func(*args, **kwargs)
            finally:
                signal.alarm(0)
            
            return result
        return wrapper
    return decorator


class PerformanceTimer:
    """Context manager for measuring execution time."""
    
    def __init__(self, name: str = "operation"):
        self.name = name
        self.start_time = None
        self.end_time = None
        self.duration = None
    
    def __enter__(self):
        self.start_time = time.perf_counter()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.perf_counter()
        self.duration = self.end_time - self.start_time
    
    def __repr__(self):
        if self.duration is not None:
            return f"PerformanceTimer({self.name}): {self.duration:.4f}s"
        return f"PerformanceTimer({self.name}): not measured"


class MockVerilogGenerator:
    """Mock Verilog code generator for testing."""
    
    def __init__(self):
        self.generated_modules = []
    
    def generate_module(
        self, 
        module_name: str, 
        inputs: List[str], 
        outputs: List[str],
        parameters: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generate a mock Verilog module."""
        params = parameters or {}
        
        verilog_code = f"""
module {module_name} (
    input wire clk,
    input wire rst_n,
{self._format_ports(inputs, 'input')},
{self._format_ports(outputs, 'output')}
);

{self._generate_parameters(params)}

    // Generated by MockVerilogGenerator
    // This is a placeholder implementation
    
    always @(posedge clk or negedge rst_n) begin
        if (!rst_n) begin
            // Reset logic
        end else begin
            // Functional logic placeholder
        end
    end

endmodule
"""
        
        self.generated_modules.append({
            'name': module_name,
            'code': verilog_code,
            'inputs': inputs,
            'outputs': outputs,
            'parameters': params
        })
        
        return verilog_code
    
    def _format_ports(self, ports: List[str], direction: str) -> str:
        """Format port declarations."""
        if not ports:
            return ""
        
        formatted = []
        for port in ports:
            formatted.append(f"    {direction} wire [{port}_WIDTH-1:0] {port}")
        
        return ",\n".join(formatted)
    
    def _generate_parameters(self, params: Dict[str, Any]) -> str:
        """Generate parameter declarations."""
        if not params:
            return ""
        
        param_lines = []
        for name, value in params.items():
            param_lines.append(f"    parameter {name} = {value};")
        
        return "\n".join(param_lines) + "\n"


def assert_verilog_syntax(verilog_code: str) -> bool:
    """Basic Verilog syntax validation for testing."""
    required_keywords = ["module", "endmodule"]
    for keyword in required_keywords:
        if keyword not in verilog_code:
            return False
    
    # Check for balanced parentheses
    paren_count = 0
    for char in verilog_code:
        if char == '(':
            paren_count += 1
        elif char == ')':
            paren_count -= 1
        if paren_count < 0:
            return False
    
    return paren_count == 0


def compare_model_outputs(
    model1: torch.nn.Module, 
    model2: torch.nn.Module, 
    test_input: torch.Tensor,
    tolerance: float = 1e-5
) -> bool:
    """Compare outputs of two models for equivalence testing."""
    model1.eval()
    model2.eval()
    
    with torch.no_grad():
        output1 = model1(test_input)
        output2 = model2(test_input)
    
    return torch.allclose(output1, output2, atol=tolerance)


def generate_test_data(
    shape: tuple, 
    data_type: str = "random",
    seed: Optional[int] = None
) -> torch.Tensor:
    """Generate test data with specified characteristics."""
    if seed is not None:
        torch.manual_seed(seed)
    
    if data_type == "random":
        return torch.randn(*shape)
    elif data_type == "ones":
        return torch.ones(*shape)
    elif data_type == "zeros":
        return torch.zeros(*shape)
    elif data_type == "sequential":
        return torch.arange(np.prod(shape)).float().reshape(shape)
    else:
        raise ValueError(f"Unknown data type: {data_type}")


class ResourceMonitor:
    """Monitor system resources during test execution."""
    
    def __init__(self):
        self.start_memory = None
        self.end_memory = None
        self.peak_memory = None
        self.start_time = None
        self.end_time = None
    
    def __enter__(self):
        try:
            import psutil
            process = psutil.Process()
            self.start_memory = process.memory_info().rss / 1024 / 1024  # MB
            self.start_time = time.time()
        except ImportError:
            pass
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        try:
            import psutil
            process = psutil.Process()
            self.end_memory = process.memory_info().rss / 1024 / 1024  # MB
            self.end_time = time.time()
        except ImportError:
            pass
    
    @property
    def memory_delta(self) -> Optional[float]:
        """Memory usage change in MB."""
        if self.start_memory is not None and self.end_memory is not None:
            return self.end_memory - self.start_memory
        return None
    
    @property
    def execution_time(self) -> Optional[float]:
        """Execution time in seconds."""
        if self.start_time is not None and self.end_time is not None:
            return self.end_time - self.start_time
        return None


@contextlib.contextmanager
def temporary_env_var(name: str, value: str):
    """Temporarily set an environment variable."""
    old_value = os.environ.get(name)
    os.environ[name] = value
    try:
        yield
    finally:
        if old_value is None:
            os.environ.pop(name, None)
        else:
            os.environ[name] = old_value


def create_test_model_file(temp_dir: Path, model: torch.nn.Module, filename: str = "test_model.pth") -> Path:
    """Save a test model to a temporary file."""
    model_path = temp_dir / filename
    torch.save(model.state_dict(), model_path)
    return model_path


def assert_performance_improvement(
    baseline_time: float, 
    new_time: float, 
    min_improvement: float = 2.0
) -> None:
    """Assert that performance has improved by at least the specified factor."""
    speedup = baseline_time / new_time
    assert speedup >= min_improvement, (
        f"Performance improvement insufficient: {speedup:.2f}x < {min_improvement}x"
    )


def assert_energy_efficiency(
    baseline_energy: float, 
    new_energy: float, 
    min_efficiency: float = 10.0
) -> None:
    """Assert that energy efficiency has improved by at least the specified factor."""
    efficiency = baseline_energy / new_energy
    assert efficiency >= min_efficiency, (
        f"Energy efficiency insufficient: {efficiency:.2f}x < {min_efficiency}x"
    )


class TestDataGenerator:
    """Generate various types of test data for neural networks."""
    
    @staticmethod
    def linear_test_data(input_size: int, batch_size: int = 32) -> tuple:
        """Generate test data for linear layers."""
        inputs = torch.randn(batch_size, input_size)
        targets = torch.randn(batch_size, 1)
        return inputs, targets
    
    @staticmethod
    def conv_test_data(channels: int, height: int, width: int, batch_size: int = 8) -> tuple:
        """Generate test data for convolutional layers."""
        inputs = torch.randn(batch_size, channels, height, width)
        targets = torch.randint(0, 10, (batch_size,))
        return inputs, targets
    
    @staticmethod
    def sequence_test_data(seq_len: int, feature_size: int, batch_size: int = 16) -> tuple:
        """Generate test data for sequence models."""
        inputs = torch.randn(batch_size, seq_len, feature_size)
        targets = torch.randint(0, 2, (batch_size,))
        return inputs, targets


def validate_test_coverage(module_name: str, min_coverage: float = 80.0) -> None:
    """Validate that test coverage meets minimum requirements."""
    try:
        import coverage
        cov = coverage.Coverage()
        cov.load()
        coverage_data = cov.get_data()
        
        # This is a simplified check - in practice, you'd need more sophisticated coverage analysis
        print(f"Coverage validation for {module_name}: placeholder implementation")
        
    except ImportError:
        pytest.skip("coverage package not available")


# Utility decorators for common test patterns
slow_test = pytest.mark.slow
integration_test = pytest.mark.integration
gpu_test = pytest.mark.gpu
network_test = pytest.mark.network
performance_test = pytest.mark.performance
security_test = pytest.mark.security